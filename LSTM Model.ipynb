{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44fc96a",
   "metadata": {},
   "source": [
    "### Block 1: Importing Required Libraries\n",
    "\n",
    "This block imports all the necessary libraries for the LSTM stock prediction workflow:\n",
    "\n",
    "- **yfinance**: For downloading historical stock price data.\n",
    "- **pandas / numpy**: For data manipulation and numerical operations.\n",
    "- **time / logging**: Utility modules for handling delays and tracking process logs.\n",
    "- **matplotlib.pyplot**: For plotting data and visualizations.\n",
    "- **sklearn**: Used later for preprocessing and evaluation (e.g., MinMaxScaler, metrics).\n",
    "- **tensorflow / keras.models.load_model**: Required for loading and running the pre-trained LSTM model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93537d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data acquisition and manipulation\n",
    "# import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Utilities\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine learning and deep learning\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow\n",
    "from keras.models import load_model  # For loading pre-trained LSTM models\n",
    "\n",
    "\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5c7079",
   "metadata": {},
   "source": [
    "### Block 2: Data Download and Preprocessing Configuration (Alpha Vantage)\n",
    "\n",
    "This block retrieves historical daily stock data from the **Alpha Vantage** API and prepares it for further analysis.\n",
    "\n",
    "- **Setup**: Defines the stock ticker, date range, and CSV output filename.\n",
    "- **API Call**: Sends a request using the `TIME_SERIES_DAILY` function.\n",
    "- **Retry Logic**: Retries up to 3 times in case of failure, with a short delay between attempts.\n",
    "- **Data Processing**:\n",
    "  - Parses the JSON response into a DataFrame.\n",
    "  - Renames columns to standard financial labels.\n",
    "  - Sorts by date and resets the index.\n",
    "  - Filters required columns for modeling.\n",
    "\n",
    "- **Output**: Saves the cleaned dataset to CSV for downstream use.\n",
    "\n",
    "> ‚ö†Ô∏è Uses Alpha Vantage's free tier‚Äîsubject to rate limits. Ensure valid US stock tickers are used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83da266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2: Fetch daily data from Alpha Vantage (free TIME_SERIES_DAILY)\n",
    "ticker = 'TSLA'\n",
    "csv_file = f'{ticker}_historical_data.csv'\n",
    "csv_path = f\"{ticker}_historical_data.csv\"\n",
    "api_key = 'GTFJMDMRP40HCBL8Q'\n",
    "base_url = 'https://www.alphavantage.co/query'\n",
    "max_retries = 3\n",
    "\n",
    "params = {\n",
    "    'function': 'TIME_SERIES_DAILY',  \n",
    "    'symbol': ticker,\n",
    "    'outputsize': 'full',\n",
    "    'datatype': 'json',\n",
    "    'apikey': api_key\n",
    "}\n",
    "\n",
    "for attempt in range(max_retries):\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if 'Time Series (Daily)' not in data:\n",
    "            raise ValueError(f\"Invalid API response: {data.get('Note') or data.get('Error Message') or 'Unknown error'}\")\n",
    "\n",
    "        # Parse the JSON into a DataFrame\n",
    "        raw_data = data['Time Series (Daily)']\n",
    "        df = pd.DataFrame.from_dict(raw_data, orient='index').rename(columns={\n",
    "            '1. open': 'Open',\n",
    "            '2. high': 'High',\n",
    "            '3. low': 'Low',\n",
    "            '4. close': 'Close',\n",
    "            '5. volume': 'Volume'\n",
    "        })\n",
    "\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df = df.sort_index()\n",
    "        df = df.astype(float)\n",
    "        df.reset_index(inplace=True)\n",
    "        df.rename(columns={'index': 'Date'}, inplace=True)\n",
    "\n",
    "        # Reorder columns\n",
    "        df = df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "\n",
    "        # Save to CSV\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"‚úÖ Data saved to {csv_path}\")\n",
    "        break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Attempt {attempt + 1} failed: {e}\")\n",
    "        time.sleep(5)\n",
    "else:\n",
    "    raise RuntimeError(\"‚ùå All download attempts failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7d25eb",
   "metadata": {},
   "source": [
    "### Block 3: Load and Inspect CSV Data\n",
    "\n",
    "This block loads the previously saved historical stock data and performs basic integrity checks:\n",
    "\n",
    "- **Data Loading**:\n",
    "  - Loads the CSV using `pandas.read_csv`.\n",
    "  - Parses the 'Date' column as datetime for easier time-based operations.\n",
    "  - Uses `csv_file` as the source, which was defined earlier.\n",
    "\n",
    "- **Optional Setup**:\n",
    "  - A line to set 'Date' as the DataFrame index is present but commented out. This would be useful for time-series analysis if enabled.\n",
    "\n",
    "- **Validation and Inspection**:\n",
    "  - Prints the shape and column names of the loaded dataset.\n",
    "  - Displays the first 5 rows using `display()` (Jupyter-specific).\n",
    "  - Checks and prints the count of missing values in each column.\n",
    "\n",
    "This step ensures the data has been loaded correctly and is ready for preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8cbbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• Load historical stock data from CSV file with parsed datetime\n",
    "# Note: `csv_file` is used instead of `csv_path` for consistency\n",
    "df = pd.read_csv(csv_file, parse_dates=['Date'])\n",
    "\n",
    "# Optional: set 'Date' as index for time-series operations (currently commented out)\n",
    "# df.set_index('Date', inplace=True)\n",
    "\n",
    "# üß™ Verify data integrity\n",
    "print(f\"Loaded data shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# üñºÔ∏è Display first few records for visual inspection\n",
    "display(df.head())\n",
    "\n",
    "# üîç Optional check for missing data\n",
    "missing_counts = df.isnull().sum()\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(missing_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082e1f12",
   "metadata": {},
   "source": [
    "### Block 4: Compute 100-Day Moving Average\n",
    "\n",
    "This block calculates a simple moving average (SMA) of the stock's closing price over a 100-day window:\n",
    "\n",
    "- Uses `pandas.Series.rolling(window=100)` to create a rolling view.\n",
    "- Computes the mean for each 100-day window using `.mean()`.\n",
    "- The resulting `ma100` series aligns with the original DataFrame and can be used for plotting or trend analysis.\n",
    "\n",
    "The moving average helps smooth short-term fluctuations and highlight longer-term price trends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179b1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìà Compute 100-day moving average of the 'Close' price\n",
    "ma100 = df['Close'].rolling(window=100).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb42d09",
   "metadata": {},
   "source": [
    "### Block 5: Visualize Closing Price with 100-Day Moving Average\n",
    "\n",
    "This block creates a line chart showing the stock's closing price over time, along with its 100-day moving average:\n",
    "\n",
    "- **Line 1**: Plots the original 'Close' price.\n",
    "- **Line 2**: Overlays the 100-day moving average (`ma100`) in red (`'r'`).\n",
    "- **Aesthetics**:\n",
    "  - Sets figure size to 12√ó6 for better visibility.\n",
    "  - Adds title, axis labels, legend, and grid for clarity.\n",
    "  - Uses `tight_layout()` to ensure elements don't overlap.\n",
    "\n",
    "The chart helps identify long-term price trends and compare them to recent price movements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39689276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5: üìä Visualize Closing Price with 100-Day Moving Average\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot raw closing price\n",
    "plt.plot(df.index, df['Close'], label='Close Price')\n",
    "\n",
    "# Plot 100-day moving average in red\n",
    "plt.plot(df.index, ma100, 'r', label='100-Day MA')\n",
    "\n",
    "# Add title and axis labels\n",
    "plt.title(f'{ticker} Close Price with 100-Day Moving Average')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "\n",
    "# Add legend and grid\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Improve layout and render plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4132b82",
   "metadata": {},
   "source": [
    "### Block 6: Plot Close Price with 100-Day and 200-Day Moving Averages\n",
    "\n",
    "This block adds a longer-term moving average to the visualization and overlays it with previous data:\n",
    "\n",
    "- **Computation**:\n",
    "  - Calculates a 200-day moving average (`ma200`) using a rolling mean on the 'Close' price.\n",
    "\n",
    "- **Visualization**:\n",
    "  - Plots:\n",
    "    - Original 'Close' price.\n",
    "    - 100-day moving average in red.\n",
    "    - 200-day moving average in green.\n",
    "  - Adds titles, labels, legend, and grid for readability.\n",
    "  - Uses `tight_layout()` to prevent label clipping.\n",
    "\n",
    "The 200-day MA offers insights into long-term trends and is often used alongside the 100-day MA to detect crossover signals in technical analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e4dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6: üìä Calculate 200-Day Moving Average and Plot All Price Trends\n",
    "\n",
    "# Calculate 200-day moving average\n",
    "ma200 = df['Close'].rolling(window=200).mean()\n",
    "\n",
    "# Create figure for plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot original close prices\n",
    "plt.plot(df.index, df['Close'], label='Close Price')\n",
    "\n",
    "# Plot 100-day moving average (in red)\n",
    "plt.plot(df.index, ma100, 'r', label='100-Day MA')\n",
    "\n",
    "# Plot 200-day moving average (in green)\n",
    "plt.plot(df.index, ma200, 'g', label='200-Day MA')\n",
    "\n",
    "# Title and axis labeling\n",
    "plt.title(f'{ticker} Close Price with 100-Day and 200-Day Moving Averages')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "\n",
    "# Add legend and grid\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Layout adjustment and render\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe43e4a",
   "metadata": {},
   "source": [
    "### Block 7: Split Dataset into Training and Testing Sets\n",
    "\n",
    "This block divides the stock price data into two subsets for model development:\n",
    "\n",
    "- **Training Set**:\n",
    "  - Takes the first 70% of the data.\n",
    "  - Used for training the LSTM model.\n",
    "\n",
    "- **Testing Set**:\n",
    "  - Takes the remaining 30% of the data.\n",
    "  - Used for evaluating model performance on unseen data.\n",
    "\n",
    "- Both subsets include only the `'Close'` price series and are wrapped in separate `DataFrame` objects.\n",
    "\n",
    "- The shapes of both subsets are printed to verify correct partitioning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a03021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7: üß™ Splitting data into training and testing sets\n",
    "\n",
    "# Use 70% of the 'Close' price data for training, 30% for testing\n",
    "train_size = int(len(df) * 0.70)\n",
    "\n",
    "# Extract training data (first 70%)\n",
    "data_training = pd.DataFrame(df['Close'][0:train_size])\n",
    "\n",
    "# Extract testing data (remaining 30%)\n",
    "data_testing = pd.DataFrame(df['Close'][train_size:int(len(df))])\n",
    "\n",
    "# Output the shapes of both sets\n",
    "print(\"Training data shape:\", data_training.shape)\n",
    "print(\"Testing data shape:\", data_testing.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c840cc51",
   "metadata": {},
   "source": [
    "### Block 8: Preview Testing Data\n",
    "\n",
    "This block displays the first five rows of the testing set (`data_testing`) using `.head()`.\n",
    "\n",
    "- Allows quick visual verification of:\n",
    "  - Data integrity after the split.\n",
    "  - Value ranges and formatting.\n",
    "- Especially helpful in Jupyter notebooks for debugging and exploratory analysis.\n",
    "\n",
    "This step ensures the testing dataset is properly structured before further processing or modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626d0ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 8: üëÅÔ∏è‚Äçüó®Ô∏è Preview the first few rows of the testing data\n",
    "data_testing.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98563558",
   "metadata": {},
   "source": [
    "### Block 9: Initialize MinMaxScaler for Normalization\n",
    "\n",
    "This block sets up data normalization using scikit-learn's `MinMaxScaler`:\n",
    "\n",
    "- **Why Normalize?**\n",
    "  - Neural networks (like LSTMs) are sensitive to the scale of input data.\n",
    "  - Scaling ensures all values fall within a uniform range, improving training stability.\n",
    "\n",
    "- **Details**:\n",
    "  - `MinMaxScaler(feature_range=(0, 1))` scales features linearly to the [0, 1] range.\n",
    "  - This scaler will be applied to the training data and later reused for testing data to maintain consistency.\n",
    "\n",
    "This setup prepares the pipeline for consistent data scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f955924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 9: üîÑ Initialize MinMaxScaler for feature normalization\n",
    "# Create a MinMaxScaler instance to scale values to the range [0, 1]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0efbde",
   "metadata": {},
   "source": [
    "### Block 10: Fit and Transform Training Data Using MinMaxScaler\n",
    "\n",
    "This block applies normalization to the training data:\n",
    "\n",
    "- **fit_transform()**:\n",
    "  - Fits the `MinMaxScaler` on `data_training`, learning the min and max values.\n",
    "  - Transforms the data into the [0, 1] range accordingly.\n",
    "  \n",
    "- **Output**:\n",
    "  - Prints the scaled `numpy` array.\n",
    "  - Displays the shape of the resulting array for confirmation.\n",
    "\n",
    "This step ensures the training input data is scaled consistently before being fed into the LSTM model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7caf24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 10: üîß Fit MinMaxScaler on training data and apply transformation\n",
    "\n",
    "# Fit the scaler on training data and transform it to [0, 1] range\n",
    "data_training_array = scaler.fit_transform(data_training)\n",
    "\n",
    "# Print the scaled array (for inspection/debugging)\n",
    "print(data_training_array)\n",
    "\n",
    "# Show the shape of the transformed array\n",
    "print(data_training_array.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73caba8d",
   "metadata": {},
   "source": [
    "### Block 11: Create 100-Day Rolling Sequences for LSTM Training\n",
    "\n",
    "This block prepares the input and target arrays for training an LSTM model:\n",
    "\n",
    "- **Input (`x_train`)**:\n",
    "  - Each entry is a 100-day sequence of normalized closing prices.\n",
    "  - Shape of each input: `(100, 1)` before reshaping for LSTM.\n",
    "\n",
    "- **Target (`y_train`)**:\n",
    "  - Each target is the next closing price (i.e., the 101st value).\n",
    "\n",
    "- **Loop Explanation**:\n",
    "  - Starts from index 100 so that each input window contains 100 past values.\n",
    "  - Appends the slice `data_training_array[i-100:i]` to `x_train`.\n",
    "  - Appends the value at index `i` to `y_train`.\n",
    "\n",
    "- **Conversion**:\n",
    "  - After the loop, both `x_train` and `y_train` are converted to NumPy arrays to be used in model training.\n",
    "\n",
    "This structure enables the LSTM to learn from historical patterns in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8916cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 11: üß© Create 100-day rolling window sequences for LSTM training\n",
    "\n",
    "x_train = []  # Input sequences of shape (100, 1)\n",
    "y_train = []  # Corresponding targets (next value after 100-day window)\n",
    "\n",
    "# Iterate from index 100 to the end of the scaled training array\n",
    "for i in range(100, data_training_array.shape[0]):\n",
    "    x_train.append(data_training_array[i-100:i])  # Sequence of 100 time steps\n",
    "    y_train.append(data_training_array[i, 0])     # Target: 101st value\n",
    "\n",
    "# Convert lists to NumPy arrays for model training\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b141d0e",
   "metadata": {},
   "source": [
    "### Block 12: Import Keras Classes for LSTM Model Construction\n",
    "\n",
    "This block imports the necessary components from the Keras library to define and build an LSTM-based neural network:\n",
    "\n",
    "- **`Sequential`**:\n",
    "  - A linear stack model, where layers are added one after another.\n",
    "\n",
    "- **`LSTM`**:\n",
    "  - Long Short-Term Memory layer, ideal for learning temporal patterns in time-series data.\n",
    "\n",
    "- **`Dropout`**:\n",
    "  - Regularization layer to reduce overfitting by randomly disabling a fraction of neurons during training.\n",
    "\n",
    "- **`Dense`**:\n",
    "  - Fully connected output layer used at the end of the model to make predictions.\n",
    "\n",
    "These components will be used to define the LSTM architecture in upcoming blocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679f1fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 12: üì¶ Import essential Keras components for building the LSTM model\n",
    "\n",
    "from keras.layers import Dense, Dropout, LSTM  # Core layers for LSTM architecture\n",
    "from keras.models import Sequential           # Sequential model container\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964c2423",
   "metadata": {},
   "source": [
    "### Block 13: Define Deep Stacked LSTM Model Architecture\n",
    "\n",
    "This block constructs a deep LSTM model using Keras' Sequential API for time-series forecasting:\n",
    "\n",
    "- **Input Shape**:\n",
    "  - `(100, 1)`, corresponding to 100 past days of 1-dimensional closing price input.\n",
    "\n",
    "- **LSTM Layers**:\n",
    "  - 4 LSTM layers with increasing units (50 ‚Üí 60 ‚Üí 80 ‚Üí 120).\n",
    "  - `return_sequences=True` is used in all but the final LSTM layer to stack them.\n",
    "  - Activation function: `'relu'` to introduce non-linearity.\n",
    "\n",
    "- **Dropout Layers**:\n",
    "  - Dropout regularization is applied after each LSTM layer.\n",
    "  - Dropout rates increase progressively (0.2 ‚Üí 0.3 ‚Üí 0.4 ‚Üí 0.5) to combat overfitting.\n",
    "\n",
    "- **Dense Output Layer**:\n",
    "  - A fully connected layer with 1 unit to predict the next closing price value.\n",
    "\n",
    "This architecture is designed to capture both short- and long-range temporal dependencies in the stock price data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e730daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 13: üèóÔ∏è Define the LSTM model architecture using Keras Sequential API\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Layer 1: LSTM with 50 units, ReLU activation\n",
    "# - Input shape: (time_steps=100, features=1)\n",
    "# - return_sequences=True allows stacking more LSTM layers\n",
    "model.add(LSTM(units=50, activation='relu', return_sequences=True, input_shape=(x_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))  # Dropout to reduce overfitting\n",
    "\n",
    "# Layer 2: LSTM with 60 units, returns sequences for next layer\n",
    "model.add(LSTM(units=60, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Layer 3: LSTM with 80 units, returns sequences for next layer\n",
    "model.add(LSTM(units=80, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# Layer 4: Final LSTM layer with 120 units, returns last output only\n",
    "model.add(LSTM(units=120, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer: Fully connected Dense layer with 1 unit to predict a single value\n",
    "model.add(Dense(units=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c934e8",
   "metadata": {},
   "source": [
    "### Block 14: Display the LSTM Model Summary\n",
    "\n",
    "This block prints a detailed summary of the model architecture:\n",
    "\n",
    "- **Layer-wise Details**:\n",
    "  - Shows each layer type, output shape, and number of trainable parameters.\n",
    "\n",
    "- **Purpose**:\n",
    "  - Useful to verify:\n",
    "    - Correct stacking of LSTM and Dropout layers.\n",
    "    - Expected output dimensions at each stage.\n",
    "    - Total number of trainable parameters.\n",
    "\n",
    "This is a good checkpoint to confirm the model was built as intended before compilation and training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92ea43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 14: üìã Display the model architecture summary\n",
    "\n",
    "# Outputs the full layer-wise model structure, output shapes, and parameter counts\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db277663",
   "metadata": {},
   "source": [
    "### Block 15: Compile and Train the LSTM Model\n",
    "\n",
    "This block compiles and initiates the training process:\n",
    "\n",
    "- **Compilation**:\n",
    "  - **Optimizer**: `'adam'`, a widely used adaptive learning algorithm.\n",
    "  - **Loss Function**: `'mean_squared_error'`, ideal for regression tasks like stock price prediction.\n",
    "\n",
    "- **Training**:\n",
    "  - The model is trained on `x_train` and `y_train` for **2 epochs**.\n",
    "  - Returns a `history` object that logs training loss per epoch.\n",
    "\n",
    "> Note: 2 epochs is minimal and likely intended for testing. More epochs may be needed for convergence in production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5186ade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 15: ‚öôÔ∏è Compile and train the LSTM model\n",
    "\n",
    "# Compile the model with Adam optimizer and Mean Squared Error loss\n",
    "# - Adam is efficient and adaptive for time-series learning\n",
    "# - MSE is suitable for continuous value prediction\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# Train the model for epochs on the prepared training data\n",
    "# - Returns a history object containing loss per epoch\n",
    "history = model.fit(x_train, y_train, epochs=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15de57f9",
   "metadata": {},
   "source": [
    "### Block 16: Visualize Training Loss Over Epochs\n",
    "\n",
    "This block plots the Mean Squared Error (MSE) loss recorded during model training:\n",
    "\n",
    "- **X-axis**: Epoch number.\n",
    "- **Y-axis**: Training loss (MSE).\n",
    "- **Purpose**:\n",
    "  - Evaluate how the model's error changes during training.\n",
    "  - Detect underfitting or overfitting trends (though with only 2 epochs, patterns may be minimal).\n",
    "\n",
    "> This plot is useful to assess convergence and training quality over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0c1c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 16: üìâ Plot the model's training loss over epochs\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')  # Plot MSE loss per epoch\n",
    "plt.title('Model Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de200ec",
   "metadata": {},
   "source": [
    "### Block 17: Save the Trained LSTM Model\n",
    "\n",
    "This block saves the trained Keras model to a file:\n",
    "\n",
    "- **Filename**: `'keras_model.h5'` (HDF5 format).\n",
    "- **What is saved**:\n",
    "  - Model architecture.\n",
    "  - Trained weights.\n",
    "  - Training configuration (e.g., optimizer and loss).\n",
    "  - State of the optimizer (for resuming training if needed).\n",
    "\n",
    "This allows the model to be reloaded later for inference or continued training without retraining from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658c52f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 17: üíæ Save the trained LSTM model to a file in HDF5 format\n",
    "\n",
    "# Saves the full model architecture, weights, and optimizer state\n",
    "model.save('keras_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4708e467",
   "metadata": {},
   "source": [
    "### Block 18: Prepare Combined Dataset for LSTM Testing Phase\n",
    "\n",
    "This block constructs the input data for model inference by:\n",
    "\n",
    "- **Step 1**: Extracting the last 100 days of closing prices from the training set.\n",
    "  - These are needed as the initial input window for the LSTM model.\n",
    "\n",
    "- **Step 2**: Concatenating the extracted window with the actual testing set.\n",
    "  - This creates a seamless sequence for time-windowed prediction.\n",
    "  - `ignore_index=True` ensures a continuous index in the resulting DataFrame.\n",
    "\n",
    "This combined `final_df` will be used to form sequential test inputs in upcoming blocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67443eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 18: üß™ Prepare test dataset by combining last 100 training days with testing set\n",
    "\n",
    "# Preview the first few rows of the test set\n",
    "print(data_testing.head())\n",
    "\n",
    "# Extract last 100 closing prices from training set ‚Äî needed to form the first LSTM input window\n",
    "past_100_days = data_training.tail(100)\n",
    "\n",
    "# Concatenate the 100-day window with the testing data to form the prediction input set\n",
    "# - ignore_index=True resets the index of the resulting DataFrame\n",
    "final_df = pd.concat([past_100_days, data_testing], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb11347c",
   "metadata": {},
   "source": [
    "### Block 19: Display the First Few Rows of the Combined Dataset\n",
    "\n",
    "This block outputs the first 5 rows of `final_df`:\n",
    "\n",
    "- Helps verify that the concatenation of:\n",
    "  - The last 100 days of training data, and\n",
    "  - The test dataset,\n",
    "  was successful.\n",
    "\n",
    "This step ensures that the data structure is as expected before applying normalization and forming prediction windows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f327dd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 19: üîç Preview the combined dataset used for LSTM prediction input\n",
    "final_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b9edd8",
   "metadata": {},
   "source": [
    "### Block 20: Normalize Combined Input Data for Testing\n",
    "\n",
    "This block applies MinMax normalization to `final_df`, which includes:\n",
    "\n",
    "- **Past 100 days of training data**\n",
    "- **Full testing dataset**\n",
    "\n",
    "Steps:\n",
    "\n",
    "- **`fit_transform()`**:\n",
    "  - Fits the `MinMaxScaler` to `final_df`.\n",
    "  - Scales all values to the range [0, 1].\n",
    "\n",
    "- **Why normalize again?**  \n",
    "  - This may be acceptable if testing data is being scaled separately.\n",
    "  - In strict deployment, you‚Äôd use `scaler.transform()` to avoid data leakage, but the current code fits anew.\n",
    "\n",
    "- **Output**:\n",
    "  - Prints the normalized array.\n",
    "  - Shows the shape of the scaled dataset.\n",
    "\n",
    "This prepares the input array for generating time windows for prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3653005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 20: üî¢ Normalize the combined dataset for testing\n",
    "\n",
    "# Apply MinMax scaling to the concatenated data (100-day window + test set)\n",
    "# Note: This re-fits the scaler to the combined data, which is acceptable for test-time normalization in this context.\n",
    "input_data = scaler.fit_transform(final_df)\n",
    "\n",
    "# Display the scaled values and the shape of the resulting array\n",
    "print(input_data)\n",
    "print(input_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d35c99",
   "metadata": {},
   "source": [
    "### Block 21: Generate Time-Series Windows for Model Prediction\n",
    "\n",
    "This block constructs input-output pairs from the normalized test data:\n",
    "\n",
    "- **Window size**: 100 days (same as training)\n",
    "- **Purpose**: To form sequences that match the expected LSTM input shape\n",
    "\n",
    "Steps:\n",
    "\n",
    "- For each index `i` from 100 to the end:\n",
    "  - `x_test[i]`: Previous 100 scaled closing prices\n",
    "  - `y_test[i]`: True scaled closing price at time step `i` (used for evaluation)\n",
    "\n",
    "- Converts the lists to NumPy arrays, ready for LSTM prediction.\n",
    "\n",
    "> These sequences are now in the correct shape:  \n",
    "> `x_test.shape` ‚Üí `(num_samples, 100, 1)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaebe923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 21: üìà Create input sequences (x_test) and true labels (y_test) from scaled input_data\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "# Generate input sequences of length 100 (to match LSTM window)\n",
    "for i in range(100, input_data.shape[0]):\n",
    "    x_test.append(input_data[i-100:i])           # Sequence of 100 time steps\n",
    "    y_test.append(input_data[i, 0])              # True (scaled) closing price at position i\n",
    "\n",
    "# Convert to NumPy arrays for model prediction\n",
    "x_test, y_test = np.array(x_test), np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac94698e",
   "metadata": {},
   "source": [
    "### Block 22: Validate Test Data Format Before Prediction\n",
    "\n",
    "This block ensures that the test data is in the correct format for model input:\n",
    "\n",
    "- **Conversion**:\n",
    "  - Explicitly converts `x_test` and `y_test` to NumPy arrays (for safety).\n",
    "\n",
    "- **Validation**:\n",
    "  - Prints the shapes of both arrays to confirm:\n",
    "    - `x_test`: Should be 3D ‚Üí `(num_samples, 100, 1)` ‚Äî for LSTM input.\n",
    "    - `y_test`: Should be 1D ‚Üí `(num_samples,)` ‚Äî actual target values.\n",
    "\n",
    "This step ensures data integrity just before feeding it into the trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b61291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 22: ‚úÖ Ensure test sequences and labels are NumPy arrays and check their shapes\n",
    "\n",
    "# Convert to NumPy arrays (redundant but safe in case of previous modification)\n",
    "x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "\n",
    "# Display the shapes of the test input and output arrays\n",
    "print(\"x_test shape:\", x_test.shape)  # Expected: (samples, 100, 1)\n",
    "print(\"y_test shape:\", y_test.shape)  # Expected: (samples,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc184ab8",
   "metadata": {},
   "source": [
    "### Block 23: Predict Closing Prices Using the Trained LSTM Model\n",
    "\n",
    "This block uses the trained model to perform inference on `x_test`:\n",
    "\n",
    "- **`model.predict(x_test)`**:\n",
    "  - Generates predicted closing prices for each 100-day input window.\n",
    "  - The output (`y_predicted`) is a 2D array with shape `(num_samples, 1)`.\n",
    "\n",
    "- **Shape Validation**:\n",
    "  - Confirms that predictions align in number with the ground truth values in `y_test`.\n",
    "\n",
    "These predictions are currently **in the normalized (scaled)** form and will need to be inverse-transformed later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f2c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 23: üîÆ Use the trained LSTM model to predict on the test input sequences\n",
    "\n",
    "y_predicted = model.predict(x_test)\n",
    "\n",
    "# Display the shape of the predicted output\n",
    "print(\"y_predicted shape:\", y_predicted.shape)  # Expected: (num_samples, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8526e429",
   "metadata": {},
   "source": [
    "### Block 24: Inverse Transform Predicted and Actual Values to Original Scale\n",
    "\n",
    "Since both `y_predicted` and `y_test` are currently normalized (range [0, 1]), this block reverses the MinMax scaling:\n",
    "\n",
    "- **`scaler.scale_`**:\n",
    "  - A NumPy array containing scaling factors used by MinMaxScaler.\n",
    "  - `scaler.scale_[0]` corresponds to the feature (e.g., 'Close').\n",
    "\n",
    "- **Inverse scaling**:\n",
    "  - Multiply by `1 / scaler.scale_[0]` to revert to the original price scale.\n",
    "\n",
    "- **Why this works**:\n",
    "  - MinMaxScaler scales data via:\n",
    "    \\[\n",
    "    x_{\\text{scaled}} = \\frac{x - \\text{min}}{\\text{max} - \\text{min}}\n",
    "    \\Rightarrow x = x_{\\text{scaled}} \\times (\\text{max} - \\text{min}) + \\text{min}\n",
    "    \\]\n",
    "  - Since this example omits `.min_`, the inverse assumes the min was ~0 (a common simplification).\n",
    "\n",
    "> This step is crucial for comparing model predictions to real stock prices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd6f9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 24: üîÅ Rescale predicted and actual values to original price scale\n",
    "\n",
    "# Print the scale factor used by MinMaxScaler during fitting\n",
    "print(\"Scaler scale factor used during MinMaxScaler fit:\", scaler.scale_)  # Example: [0.0025]\n",
    "\n",
    "# Invert scaling by dividing by the corresponding scale factor\n",
    "# This restores the values to their original price range\n",
    "scale_factor = 1 / scaler.scale_[0]\n",
    "\n",
    "y_predicted = y_predicted * scale_factor\n",
    "y_test = y_test * scale_factor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd457ddb",
   "metadata": {},
   "source": [
    "### Block 25: Visualize Actual vs. Predicted Stock Prices\n",
    "\n",
    "This block generates a line plot to compare the model‚Äôs predictions with the true stock prices:\n",
    "\n",
    "- **Blue line (`y_test`)**: The actual stock prices from the test set.\n",
    "- **Red line (`y_predicted`)**: The model's predicted stock prices.\n",
    "- **X-axis**: Represents the time steps (index within test data).\n",
    "- **Y-axis**: Rescaled prices in original currency units.\n",
    "\n",
    "The visual comparison helps assess how closely the model tracks the actual market trend, identifying lag, underfit, or overfit behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4a894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 25: üìà Plot Actual vs. Predicted Stock Prices\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot actual (true) prices in blue\n",
    "plt.plot(y_test, 'b', label='Original Price')\n",
    "\n",
    "# Plot predicted prices in red\n",
    "plt.plot(y_predicted, 'r', label='Predicted Price')\n",
    "\n",
    "plt.title(f'{ticker} Stock Price: Actual vs Predicted')\n",
    "plt.xlabel('Time (test data index)')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
